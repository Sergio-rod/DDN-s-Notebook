{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd4b0a09-0971-467b-8db4-0f42feeb02fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_dx = {\n",
    "    'sigmoid': np.linspace(-10, 10, 400),\n",
    "    'logarithmic': np.linspace(0.1, 10, 400),\n",
    "    'convex': np.linspace(-10, 10, 400),\n",
    "    'linear': np.linspace(-10, 10, 400),\n",
    "    'cuadratic': np.linspace(-10, 10, 400),\n",
    "    'cube': np.linspace(-10, 10, 400)}\n",
    "\n",
    "functions = {\n",
    "    'sigmoid': (x_dx['sigmoid'], 1 / (1 + np.exp(-x_dx['sigmoid'])), 'f(x)= Ïƒ(1/1+e^x)'),\n",
    "    'logarithmic' : (x_dx['logarithmic'], np.log(x_dx['logarithmic']),'f(x) = log(x)'),\n",
    "    'convex': (x_dx['convex'], np.exp(x_dx['convex']),'f(x) = e^x'),\n",
    "    'linear': (x_dx['linear'], 2 * x_dx['linear'], 'f(x) = 2x'),\n",
    "    'cuadratic': (x_dx['cuadratic'], x_dx['cuadratic'] ** 2, 'f(x) = x^2'),\n",
    "    'cube': (x_dx['cube'], x_dx['cube'] ** 3, 'f(x) = x^3'),\n",
    "}\n",
    "\n",
    "def graph_functions(dx_functions):\n",
    "    for function,x_y_label in dx_functions.items():\n",
    "        x,y,label = x_y_label\n",
    "        plt.figure(figsize=(5,3))\n",
    "        plt.plot(x,y,label=label)\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('f(x)')\n",
    "        plt.axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "        plt.axvline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'{function}_function.png')\n",
    "        plt.close()\n",
    "\n",
    "graph_functions(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfce9f1-5ccd-4d1d-aa9c-b984c764c832",
   "metadata": {},
   "source": [
    "# Neural Networks and Deep Learning\n",
    "## Logistic Regression\n",
    "Given x, want $\\hat{y}=P(y=1 | x)$\n",
    "\n",
    "\n",
    "$x \\in\\mathbb{R}^{nx}$\n",
    "\n",
    "Parameters : $w\\in\\mathbb{R}^{nx}$ , $b\\in\\mathbb{R}$\n",
    "\n",
    "$z = w^tx+b$\n",
    "\n",
    "\n",
    "Output: $\\hat{y} = \\sigma(z)$\n",
    "\n",
    "\n",
    "The sigmoid function permits to get a number between 0 - 1 and this is the definition\n",
    "\n",
    "$\\sigma(z) = \\frac{1}{1+e^-z}$\n",
    "\n",
    "![sigmoid](sigmoid_function.png)\n",
    "\n",
    "When z is a large number then the output tends to be 1 \n",
    "\n",
    "When z is a large negative number then the output tends to be 0\n",
    "\n",
    "In summary, $\\hat{y} = \\sigma(w^tx + b)$, where  $\\sigma(z) = \\frac{1}{1+e^-z}$\n",
    "\n",
    "Which means that given ${ \\{x^1,y^1,...,x^my^m}\\}$, want $ \\hat{y} \\approx{y^i} $\n",
    "\n",
    "### Logistic Cost Function or Error Function\n",
    "\n",
    "Loss(error) Function Permits evaluate how well is performing a model for a given $\\hat{y}$ against $y$\n",
    "\n",
    "Thus $L(\\hat{y},y) = \\frac{1}{2}(\\hat{y}-y)^2$\n",
    "\n",
    "This is usefull when the optimization problem is convex, due to the convexity of the formula, guarantee that any local minima is the global minima as well, make it easier the optimization process.\n",
    "\n",
    "A convex optimization problem has this graphical form:\n",
    "\n",
    "![convex_fun](convex_function.png)\n",
    "\n",
    "But for problems like logistic regresion which is a problem of classification and not of regression, we will have multiple local minima,thus is better use the logaritmic loss function.\n",
    "\n",
    "$L(\\hat{y},y) = - (y \\cdot log(\\hat{y})+(1-y) \\cdot log(1-\\hat{y}))$\n",
    "\n",
    "A logaritmic optimization problem has this graphical form:\n",
    "\n",
    "![logarithmic_func](logarithmic_function.png)\n",
    "\n",
    "Summarizing:\n",
    "\n",
    "It is important to reduce as poosible the square error, as bigger the square error means that the model is performing bad.\n",
    "\n",
    "Following this in the formula:\n",
    "\n",
    "$L(\\hat{y},y) = - (y \\cdot log(\\hat{y})+(1-y) \\cdot log(1-\\hat{y}))$\n",
    "\n",
    "If $y=1: L(\\hat{y},y) = -log(\\hat{y})$ <--- Want $log(y)$ large, want $\\hat{y}$ large.\n",
    "\n",
    "If $y=0: L(\\hat{y},y) = -((1-y) \\cdot log(1-\\hat{y})) \\rightarrow -log(1-\\hat{y}))  $ <--- Want $log(1-\\hat{y})$ large ... Want $\\hat{y}$ small. \n",
    "\n",
    "\n",
    "#### Cost Function\n",
    "\n",
    "$J(w,b) = \\frac{1}{m} \\sum_{i=1}^m L(\\hat{y}^i,y^i) = - \\frac{1}{m} \\sum_{i=1}^m[y^i \\cdot log(\\hat{y}^i) + (1-y^i)\\cdot log(1-\\hat{y}^i)]$\n",
    "\n",
    "## Derivatives\n",
    "\n",
    "In mathematics, the derivative is a fundamental concept that measures the change of a function's output for a given x.\n",
    "\n",
    "When we talk about  cartesian plan, usually is represented with two axis, x and y, this axis permit us to locate a point with coordenates (x,y)\n",
    "\n",
    "With derivates is little similar, we can locate an especific point by (x,f(x)), but this powerfull technique permit us to measure the exchange rate for a given x.\n",
    "\n",
    "$x$ = The value in the axis x\n",
    "$f(x)$ = This represents how will be handeled the input, is the pattern that will define the graph behaviour \n",
    "\n",
    "### Linear Regresion\n",
    "\n",
    "For example, to calculate a linear model, we know that the exchange rate for each x, in f(x) it will be constant, because the model is linear.\n",
    "\n",
    "For a given function: $f(x)=2x$\n",
    "\n",
    "When $x=2, f(2) \\rightarrow 2 \\cdot 2 \\rightarrow 4$\n",
    "\n",
    "Now $x=2.002, f(2.002) \\rightarrow 2 \\cdot 2.002 \\rightarrow 4.004$\n",
    "\n",
    "The slope for both graphs is equivalent to $\\frac{2}{1}$, which means that in each point of x, y will be the double of x.\n",
    "\n",
    "![linear_func](linear_function.png)\n",
    "\n",
    "### More functions\n",
    "Now, understanding that, in ML there are a lot of kind of relationships in the data, could be the scenario in which we do not have a linear relation, but cuadratic.\n",
    "\n",
    "Understanding the same concept\n",
    "\n",
    "For a given function: $f(x)=x^2$\n",
    "\n",
    "We have:  $x=2, f(2) \\rightarrow 2^2 \\rightarrow 4$\n",
    "\n",
    "When:   $x=2.001, f(2.001) \\rightarrow 2.001^2 \\rightarrow 4.004$\n",
    "\n",
    "Then the slope(derivate) at x=2 is 4, because the value increase 4 from .001 -> .004\n",
    "\n",
    "$\\frac{d}{dx} \\cdot f(x) = 4$ when $x=2$\n",
    "\n",
    "We have $x=5, f(5) \\rightarrow 5^2 \\rightarrow 25$\n",
    "\n",
    "\n",
    "When:   $x=5.001, f(5.001) \\rightarrow 5.001^2 \\rightarrow 25.010$\n",
    "\n",
    "Then the slope(derivate) at x=5 is 10, because the value increase in a rate of 10 from .001 -> .010\n",
    "\n",
    "$\\frac{d}{dx} \\cdot f(x) = 10$ when $x=5$\n",
    "\n",
    "![cuadratic_func](cuadratic_function.png)\n",
    "\n",
    "\n",
    "We have $x=5, f(5) \\rightarrow 5^3 \\rightarrow 125$\n",
    "\n",
    "\n",
    "When:   $x=5.001, f(5.001) \\rightarrow 5.001^3 \\rightarrow 125.075$\n",
    "\n",
    "Then the slope(derivate) at x=5 is 75, because the value increase in a rate of 75 from .001 -> .070\n",
    "\n",
    "$\\frac{d}{dx} \\cdot f(x) = 75$ when $x=5$\n",
    "\n",
    "![cube_func](cube_function.png)\n",
    "\n",
    "\n",
    "### Computation Graph\n",
    "\n",
    "A computation graph, is basically the calculation of the cost function in which we will try that the final value gets closer to 0 as we can.\n",
    "\n",
    "\n",
    "This starts with something called forward propagation, which simply means the calculation of simple blocks of equations followed by more blocks of equations that involve previous equations, until culminate in the final result, also know as cost function. Once we have the final result, depending if the result is positive or negative, the baiases and weights of the neural networks will be adjusted in order to aproach to 0, if is positive, then the weights and baisases will be adjusted to drag the result to the left (in a number line), if is negative, than the oposite; This is called backward propagation, because it uses the final result, to adjust the previous steps.\n",
    "\n",
    "\n",
    "In a simple example:\n",
    "$J = 3(a + bc) $\n",
    "\n",
    "The calculation of this entire equation(cost functiuon) can be splitted in several steps:\n",
    "\n",
    "1. **Calculus of $u$:**\n",
    "   \n",
    "   $u = b \\cdot c$\n",
    "\n",
    "2. **Calculus of $v$:**\n",
    "   \n",
    "   $v = a + u$\n",
    "\n",
    "3. **Calculus of $J$:**\n",
    "   \n",
    "   $J = 3 \\cdot v$\n",
    "\n",
    "## Derivatives with a Computation Grap\n",
    "\n",
    "![comp_graph](comp_graph.jpg)\n",
    "   \n",
    "\n",
    "Lets break down the meaning of a derivate with respect another derivate.\n",
    "\n",
    "In the practice the Computation Graph will make all the calculus of the forward and back propragation(propagation to the left and right), automatically. However it is important to understand how it works and how are ajusted the values of the weights anda biases along the process.\n",
    "\n",
    "As we mentioned earlier, the first step of this process, is assign arbitrary and low values to the biases and weights of the neural network. \n",
    "\n",
    "For the formula: $J = 3(a + bc) $\n",
    "\n",
    "We will use the arbitrary and low values:\n",
    "\n",
    "- $ a = 5$\n",
    "- $ b = 3$\n",
    "- $ c = 2$\n",
    "\n",
    "If we solve with those values we will get:\n",
    "\n",
    " 1. $ u = b \\cdot c \\rightarrow 6 = 3 \\cdot 2 \\ $\n",
    " 2. $ v = a + u \\rightarrow 11 = 5 + 6$\n",
    " 3. $ J = 3 \\cdot v \\rightarrow 33 = 3 \\cdot 11$\n",
    "\n",
    "At this point this is called right propagation, or forward propagation, because we are solving from the left to the right until culminate with the whole equation.\n",
    "\n",
    "Once we have done with the  forwardpropagation, we will get the final value, that will represent many things, but for now will only considered the cost function, so, the final value will represent the cost function. As is positive its needed to adjust the weights and biases to get a result closer to 0. Is there when enters the second step, backpropagation.\n",
    "\n",
    "In this step will be calculated how the derivatives affects the final result, and others results in order to know if decrease or increase biases or weights to reduce the final value.\n",
    "\n",
    "In this case, lets suppose that we want to know how changing $a$ affects the derivative of $J$. For that we calcule, for instance:\n",
    "\n",
    "New value of $a = 5.001$ then with respect to $J$ will be $J = 3 \\cdot 11.001 \\rightarrow 33.003$ so in this case $\\frac{dJ}{da} = 3$ because increase 3 times the value\n",
    "\n",
    "Another example will be perturb the value of $b = 3.001$ to observe the resulting change in $J$ represented by $\\frac{dJ}{db}$ Please notice that in this case is chained, because first of all we need to resolve for $u$, and then for $v$, and finally for $J$.\n",
    "\n",
    "Giving the equation is must resolve $ \\boxed{u = b \\cdot c} \\rightarrow \\boxed{v = a + u} \\rightarrow \\boxed{J= 3 \\cdot v} $\n",
    "\n",
    "So will we take in consideration the increment or decrement since the value that we choose to change, until the respect of.\n",
    "\n",
    "If we reeplace $b = 3.001$ we will have $ \\boxed{6.002 = 3.001 \\cdot 2} \\rightarrow \\boxed{v = 5 + 6.002} \\rightarrow \\boxed{33.006= 3 \\cdot 11.002} $\n",
    "\n",
    "So in total the decimal of $b$ corresponding to $.001$ increase to $.006$ in $J \\rightarrow \\frac{dJ}{db} = 6$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Logistic Regression Gradient Descent\n",
    "\n",
    "Giving \n",
    "\n",
    "- $x_1$\n",
    "- $w_1$\n",
    "- $x_2$\n",
    "- $w_2$\n",
    "- $b$\n",
    "\n",
    "We can calculate $\\boxed{ z = w_1 \\cdot x_1 + w_2 \\cdot x_2} \\rightarrow \\boxed{a = \\sigma(z)} \\rightarrow \\boxed{L(a,y)}$\n",
    "\n",
    "First of all we would like to calculate the result if we do a perturbation in $a$ to see the resulting change in $L$\n",
    "\n",
    "So we denote: $\\boxed{da = \\frac{dL(a,y)}{da}}  \\rightarrow \\boxed{ -\\frac{y}{a}+\\frac{1-y}{1-a}}$\n",
    "\n",
    "And also denote : $dz = \\boxed{\\frac{dL}{dz}} \\rightarrow \\boxed{\\frac{dL(a,y)}{dz}} \\rightarrow \\boxed{a-y} \\rightarrow \\boxed{\\frac{dL}{da} \\cdot \\frac{da}{dz}} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428eb60e-a940-45ac-8c7c-1555652fec4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49652384-ca5f-4f06-aa8f-abf1b39d8105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log base 2 de 8 es: 3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculando el logaritmo base 2 de 8 usando numpy\n",
    "number = 8\n",
    "log_base_2 = np.log(number) / np.log(2)\n",
    "\n",
    "print(\"log base 2 de 8 es:\", log_base_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d015c334-aeaa-453a-a13c-b1f6e8ad641e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0794415416798357"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba5ae324-5f56-421b-92fa-6d56621f3455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba2700d-5aee-4dec-a090-cd361a4f3f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

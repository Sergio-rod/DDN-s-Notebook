{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d7b22b9-501b-487c-b9c8-65b09a9eae5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39me\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd4b0a09-0971-467b-8db4-0f42feeb02fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_dx = {\n",
    "    'sigmoid': np.linspace(-10, 10, 400),\n",
    "    'logarithmic': np.linspace(0.1, 10, 400),\n",
    "    'convex': np.linspace(-10, 10, 400),\n",
    "    'linear': np.linspace(-10, 10, 400),\n",
    "    'cuadratic': np.linspace(-10, 10, 400),\n",
    "    'cube': np.linspace(-10, 10, 400)\n",
    "}\n",
    "\n",
    "functions = {\n",
    "    'sigmoid': (x_dx['sigmoid'], 1 / (1 + np.exp(-x_dx['sigmoid'])), ''),\n",
    "    'logarithmic' : (x_dx['logarithmic'], np.log(x_dx['logarithmic']),'f(x) = log(x)'),\n",
    "    'convex': (x_dx['convex'], np.exp(x_dx['convex']),'f(x) = e^x'),\n",
    "    'linear': (x_dx['linear'], 2 * x_dx['linear'], 'f(x) = 2x'),\n",
    "    'cuadratic': (x_dx['cuadratic'], x_dx['cuadratic'] ** 2, 'f(x) = x^2'),\n",
    "    'cube': (x_dx['cube'], x_dx['cube'] ** 3, 'f(x) = x^3')\n",
    "}\n",
    "\n",
    "def graph_functions(dx_functions):\n",
    "    for function,x_y_label in dx_functions.items():\n",
    "        x,y,label = x_y_label\n",
    "        plt.figure(figsize=(5,3))\n",
    "        plt.plot(x,y,label=label)\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('f(x)')\n",
    "        plt.axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "        plt.axvline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'{function}_function.png')\n",
    "        plt.close()\n",
    "\n",
    "graph_functions(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfce9f1-5ccd-4d1d-aa9c-b984c764c832",
   "metadata": {},
   "source": [
    "# Neural Networks and Deep Learning\n",
    "## Logistic Regression\n",
    "Given x, want $\\hat{y}=P(y=1 | x)$\n",
    "\n",
    "\n",
    "$x \\in\\mathbb{R}^{nx}$\n",
    "\n",
    "Parameters : $w\\in\\mathbb{R}^{nx}$ , $b\\in\\mathbb{R}$\n",
    "\n",
    "$z = w^tx+b$\n",
    "\n",
    "\n",
    "Output: $\\hat{y} = \\sigma(z)$\n",
    "\n",
    "\n",
    "The sigmoid function permits to get a number between 0 - 1 and this is the definition\n",
    "\n",
    "$\\sigma(z) = \\frac{1}{1+e^-z}$\n",
    "\n",
    "![sigmoid](sigmoid.png)\n",
    "\n",
    "When z is a large number then the output tends to be 1 \n",
    "\n",
    "When z is a large negative number then the output tends to be 0\n",
    "\n",
    "In summary, $\\hat{y} = \\sigma(w^tx + b)$, where  $\\sigma(z) = \\frac{1}{1+e^-z}$\n",
    "\n",
    "Which means that given ${ \\{x^1,y^1,...,x^my^m}\\}$, want $ \\hat{y} \\approx{y^i} $\n",
    "\n",
    "### Logistic Cost Function or Error Function\n",
    "\n",
    "Loss(error) Function Permits evaluate how well is performing a model for a given $\\hat{y}$ against $y$\n",
    "\n",
    "Thus $L(\\hat{y},y) = \\frac{1}{2}(\\hat{y}-y)^2$\n",
    "\n",
    "This is usefull when the optimization problem is convex, due to the convexity of the formula, guarantee that any local minima is the global minima as well, make it easier the optimization process.\n",
    "\n",
    "A convex optimization problem has this graphical form:\n",
    "\n",
    "![convex_fun](convex_function.png)\n",
    "\n",
    "But for problems like logistic regresion which is a problem of classification and not of regression, we will have multiple local minima,thus is better use the logaritmic loss function.\n",
    "\n",
    "$L(\\hat{y},y) = - (y \\cdot log(\\hat{y})+(1-y) \\cdot log(1-\\hat{y}))$\n",
    "\n",
    "A logaritmic optimization problem has this graphical form:\n",
    "\n",
    "![logarithmic_func](logarithmic_function.png)\n",
    "\n",
    "Summarizing:\n",
    "\n",
    "It is important to reduce as poosible the square error, as bigger the square error means that the model is performing bad.\n",
    "\n",
    "Following this in the formula:\n",
    "\n",
    "$L(\\hat{y},y) = - (y \\cdot log(\\hat{y})+(1-y) \\cdot log(1-\\hat{y}))$\n",
    "\n",
    "If $y=1: L(\\hat{y},y) = -log(\\hat{y})$ <--- Want $log(y)$ large, want $\\hat{y}$ large.\n",
    "\n",
    "If $y=0: L(\\hat{y},y) = -((1-y) \\cdot log(1-\\hat{y})) \\rightarrow -log(1-\\hat{y}))  $ <--- Want $log(1-\\hat{y})$ large ... Want $\\hat{y}$ small. \n",
    "\n",
    "\n",
    "#### Cost Function\n",
    "\n",
    "$J(w,b) = \\frac{1}{m} \\sum_{i=1}^m L(\\hat{y}^i,y^i) = - \\frac{1}{m} \\sum_{i=1}^m[y^i \\cdot log(\\hat{y}^i) + (1-y^i)\\cdot log(1-\\hat{y}^i)]$\n",
    "\n",
    "## Derivatives\n",
    "\n",
    "In mathematics, the derivative is a fundamental concept that measures the change of a function's output for a given x.\n",
    "\n",
    "When we talk about  cartesian plan, usually is represented with two axis, x and y, this axis permit us to locate a point with coordenates (x,y)\n",
    "\n",
    "With derivates is little similar, we can locate an especific point by (x,f(x)), but this powerfull technique permit us to measure the exchange rate for a given x.\n",
    "\n",
    "$x$ = The value in the axis x\n",
    "$f(x)$ = This represents how will be handeled the input, is the pattern that will define the graph behaviour \n",
    "\n",
    "### Linear Regresion\n",
    "\n",
    "For example, to calculate a linear model, we know that the exchange rate for each x, in f(x) it will be constant, because the model is linear.\n",
    "\n",
    "For a given function: $f(x)=2x$\n",
    "\n",
    "When $x=2, f(2) \\rightarrow 2 \\cdot 2 \\rightarrow 4$\n",
    "\n",
    "Now $x=2.002, f(2.002) \\rightarrow 2 \\cdot 2.002 \\rightarrow 4.004$\n",
    "\n",
    "The slope for both graphs is equivalent to $\\frac{2}{1}$, which means that in each point of x, y will be the double of x.\n",
    "\n",
    "![linear_func](linear_function.png)\n",
    "\n",
    "\n",
    "Now, understanding that, in ML there are a lot of kind of relationships in the data, could be the scenario in which we do not have a linear relation, but cuadratic.\n",
    "\n",
    "Understanding the same concept\n",
    "\n",
    "For a given function: $f(x)=x^2$\n",
    "\n",
    "We have:  $x=2, f(2) \\rightarrow 2^2 \\rightarrow 4$\n",
    "\n",
    "When:   $x=2.001, f(2.001) \\rightarrow 2.001^2 \\rightarrow 4.004$\n",
    "\n",
    "Then the slope(derivate) at x=2 is 4, because the value increase 4 from .001 -> .004\n",
    "\n",
    "$\\frac{d}{dx} \\cdot f(x) = 4$ when $x=2$\n",
    "\n",
    "We have $x=5, f(5) \\rightarrow 5^2 \\rightarrow 25$\n",
    "\n",
    "\n",
    "When:   $x=5.001, f(5.001) \\rightarrow 5.001^2 \\rightarrow 25.010$\n",
    "\n",
    "Then the slope(derivate) at x=5 is 10, because the value increase 10 from .001 -> .010\n",
    "\n",
    "$\\frac{d}{dx} \\cdot f(x) = 10$ when $x=5$\n",
    "\n",
    "![cuadratic_func](cuadratic_function.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428eb60e-a940-45ac-8c7c-1555652fec4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39e01b3-b509-4b26-9a11-4fb60fe16d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
